{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86486fc-4fcd-4b3b-9231-cbd5b761850c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "corrupt_spark_records = SparkSession.builder.appName(\"handle corrupt in Apache Spark\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8f6fa02-2d4e-42a6-a8d1-f25719e449c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = StructType([\n",
    "    StructField(\"Emp_Id\", IntegerType(), True),\n",
    "    StructField(\"Emp_name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "# /FileStore/tables/corrupt_records.csv\n",
    "emp_df = corrupt_spark_records.read.format(\"csv\")\\\n",
    "    .option(\"inferschema\", True)\\\n",
    "    .option(\"header\", True)\\\n",
    "    .schema(columns)\\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\\\n",
    "    .load(\"/FileStore/tables/corrupt_records.csv\")\n",
    "\n",
    "emp_df.show(truncate=False)\n",
    "emp_df.printSchema()\n",
    "\n",
    "\n",
    "# Using filtering to filter data - By Filtering the null value of the _corrupt_record column we can get cleaned data or proper data. so that we can proceed with further processing.\n",
    "\n",
    "emp_df.where(col(\"_corrupt_record\").isNull()).drop(\"_corrupt_record\").show()\n",
    "\n",
    "# Filter Corrupted records\n",
    "\n",
    "emp_df.where(col(\"_corrupt_record\").isNotNull()).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "966d3fbc-b502-4864-8b58-5aab694a1005",
     "showTitle": true,
     "title": "Dealing With Bad or Corrupt Records using diff Modes in PySpark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode -> PERMISSIVE\nMode -> DROPMALFORMED\nMode -> FailFast\n"
     ]
    }
   ],
   "source": [
    "#  Dealing With Bad or Corrupt Records in Apache Spark\n",
    "#  /FileStore/tables/bad_records_handling.csv\n",
    "\n",
    "data_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "#  To deal with these cases, we have the following option:\n",
    "\n",
    "# 1. PERMISSIVE : This is the default mode. Spark will load and process both complete and corrupted data, but for corrupted data it will store null. \n",
    "print(\"Mode -> PERMISSIVE\")\n",
    "corrupt_record_PERMISSIVE = corrupt_spark_records.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"mode\", \"PERMISSIVE\")\\\n",
    "    .schema(data_schema)\\\n",
    "    .load(\"/FileStore/tables/bad_records_handling.csv\")\n",
    "\n",
    "corrupt_record_PERMISSIVE.show(truncate=False)\n",
    "\n",
    "# 2. DROPMALFORMED : This mode will drop the corrupted records and will only show the correct records.\n",
    "\n",
    "print(\"Mode -> DROPMALFORMED\")\n",
    "corrupt_record_DROPMALFORMED = corrupt_spark_records.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "    .schema(data_schema)\\\n",
    "    .load(\"/FileStore/tables/bad_records_handling.csv\")\n",
    "\n",
    "corrupt_record_DROPMALFORMED.show(truncate=False)\n",
    "\n",
    "# 3. FailFast: In this mode, Spark throws an exception and halts the data loading process when it finds any bad or corrupted records.\n",
    "\n",
    "print(\"Mode -> FailFast\")\n",
    "corrupt_record_FailFast = corrupt_spark_records.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"mode\", \"FailFast\")\\\n",
    "    .schema(data_schema)\\\n",
    "    .load(\"/FileStore/tables/bad_records_handling.csv\")\n",
    "\n",
    "corrupt_record_FailFast.show(truncate=False)\n",
    "\n",
    "# 4. columnNameOfCorruptRecord Option : This will Store all the corrupted records in new column. This extra column must be defined in schema.\n",
    "\n",
    "columnNameOfCorruptRecord_df = corrupt_spark_records.read.format(\"csv\")\\\n",
    "    .schema(data_schema)\\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"corrupt_record\")\\\n",
    "    .load(\"/FileStore/tables/bad_records_handling.csv\")\n",
    "\n",
    "columnNameOfCorruptRecord_df.show(truncate=False)\n",
    "\n",
    "# 5.badRecordsPath: Spark processes only the correct records and corrupted or bad records are excluded. Corrupted or bad records will be stored in a file at the badRecordsPath location.\n",
    "\n",
    "badRecordsPath_df = corrupt_spark_records.read.format(\"csv\")\\\n",
    "    .schema(data_schema)\\\n",
    "    .option(\"badRecordsPath\", \"/FileStore/\")\\\n",
    "    .load(\"/FileStore/tables/bad_records_handling.csv\")\n",
    "\n",
    "badRecordsPath_df.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1d64341-9e5c-44e2-ad03-a4cdadd323cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/20231102T090905/bad_records/part-00000-9c447e30-98a5-4233-ad19-f546ad42c7dc</td><td>part-00000-9c447e30-98a5-4233-ad19-f546ad42c7dc</td><td>472</td><td>1698916147000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/20231102T090905/bad_records/part-00000-9c447e30-98a5-4233-ad19-f546ad42c7dc",
         "part-00000-9c447e30-98a5-4233-ad19-f546ad42c7dc",
         472,
         1698916147000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /FileStore/20231102T090905/bad_records/part-00000-9c447e30-98a5-4233-ad19-f546ad42c7dc/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2633171772153112,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "handle corrupt or bad record in PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
