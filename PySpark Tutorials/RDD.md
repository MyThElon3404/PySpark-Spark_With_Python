# RDD - Resilient Distributed Datasets
#### Resilient Distributed Datasets (RDDs) are the building blocks of PySpark. RDD is the fundamental data structure of Apache Spark. It is an immutable collection of objects that can be split across the cluster facilitating in-memory data storage thus making it more efficient. RDD being a schema-less data structure, can handle structured as well as unstructured data. While RDDs can be logically partitioned across various nodes in a cluster, operations on RDDs can also be split and executed in parallel, facilitating faster and more scalable parallel processing. RDD is a distributed memory abstraction, and one of its salient features is that it is highly resilient. This is because the data chunks are replicated across multiple nodes in the cluster and thus, they can recover quickly from any issue as the data will still be processed by other nodes even if one executor node fails, making it highly fault tolerant. Also, RDDs follow lazy evaluation, that is, the execution is not started right away, but rather they are triggered only when needed. Two basic and important operations can be carried out on RDDs â€” Transformations and Actions.

## Transformations - #### Transformations are operations on the RDDs that create a new RDD by making changes to the original RDD. Briefly, they are functions that take an existing RDD as input to give new RDDs as output, without making changes to the original RDD (Note that RDDs are immutable!) This process of transforming an RDD to a new one is done through operations such as filter, map, reduceByKey, sortBy etc.
