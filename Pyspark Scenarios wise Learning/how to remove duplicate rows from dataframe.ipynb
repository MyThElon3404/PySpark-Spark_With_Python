{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8e2571b-0d8e-47f5-b557-5f14faf97f0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1055290162342735#setting/sparkui/1206-082332-f4fvtaup/driver-6912329078628543483\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1055290162342735#setting/sparkui/1206-082332-f4fvtaup/driver-6912329078628543483\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# it's defau;t spark session in Jupiter Notebook\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a98ed12-cf4f-434c-a4ac-985195a66c4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1300737379233180>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mput\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/scenarios/duplicates.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[38;5;124;43mid,name,loc,updated_date\u001B[39;49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m1,ravi,bangalore,2021-01-01\u001B[39;49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m1,ravi,chennai,2022-02-02\u001B[39;49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m1,ravi,Hyderabad,2022-06-10\u001B[39;49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m2,Raj,bangalore,2021-01-01\u001B[39;49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m2,Raj,chennai,2022-02-02\u001B[39;49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m3,Raj,Hyderabad,2022-06-10\u001B[39;49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m4,Prasad,bangalore,2021-01-01\u001B[39;49m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124;43m5,Mahesh,chennai,2022-02-02\u001B[39;49m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;124;43m4,Prasad,Hyderabad,2022-06-10\u001B[39;49m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o1794.put.\n",
       ": org.apache.hadoop.fs.FileAlreadyExistsException: s3a://databricks-prod-storage-oregon/devtierprod1/1055290162342735/scenarios/duplicates.csv already exists\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:124)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendNonIdempotent(DbfsClient.scala:82)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.createUnbuffered(DatabricksFileSystemV1.scala:127)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.create(DatabricksFileSystemV1.scala:109)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:139)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$3(DBUtilsCore.scala:446)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$2(DBUtilsCore.scala:443)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$1(DBUtilsCore.scala:443)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:557)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:673)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:647)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:566)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:557)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:527)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.put(DBUtilsCore.scala:442)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-1300737379233180>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mput\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/scenarios/duplicates.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[38;5;124;43mid,name,loc,updated_date\u001B[39;49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m1,ravi,bangalore,2021-01-01\u001B[39;49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m1,ravi,chennai,2022-02-02\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m1,ravi,Hyderabad,2022-06-10\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m2,Raj,bangalore,2021-01-01\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m2,Raj,chennai,2022-02-02\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m3,Raj,Hyderabad,2022-06-10\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43m4,Prasad,bangalore,2021-01-01\u001B[39;49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;43m5,Mahesh,chennai,2022-02-02\u001B[39;49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;43m4,Prasad,Hyderabad,2022-06-10\u001B[39;49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:362\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    361\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 362\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o1794.put.\n: org.apache.hadoop.fs.FileAlreadyExistsException: s3a://databricks-prod-storage-oregon/devtierprod1/1055290162342735/scenarios/duplicates.csv already exists\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:124)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendNonIdempotent(DbfsClient.scala:82)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.createUnbuffered(DatabricksFileSystemV1.scala:127)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.create(DatabricksFileSystemV1.scala:109)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:139)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$3(DBUtilsCore.scala:446)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:145)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$2(DBUtilsCore.scala:443)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$1(DBUtilsCore.scala:443)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:557)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:673)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:647)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:566)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:69)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:557)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:527)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:69)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:133)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.put(DBUtilsCore.scala:442)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.hadoop.fs.FileAlreadyExistsException: s3a://databricks-prod-storage-oregon/devtierprod1/1055290162342735/scenarios/duplicates.csv already exists",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.put(\"/scenarios/duplicates.csv\", \"\"\"id,name,loc,updated_date\n",
    "1,ravi,bangalore,2021-01-01\n",
    "1,ravi,chennai,2022-02-02\n",
    "1,ravi,Hyderabad,2022-06-10\n",
    "2,Raj,bangalore,2021-01-01\n",
    "2,Raj,chennai,2022-02-02\n",
    "3,Raj,Hyderabad,2022-06-10\n",
    "4,Prasad,bangalore,2021-01-01\n",
    "5,Mahesh,chennai,2022-02-02\n",
    "4,Prasad,Hyderabad,2022-06-10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc31c79d-d856-478a-8ad0-0cb137c506ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/scenarios/duplicates.csv</td><td>duplicates.csv</td><td>274</td><td>1701845455000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/scenarios/duplicates.csv",
         "duplicates.csv",
         274,
         1701845455000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /scenarios/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a55f29-47c0-447f-a660-7ba9cf01e5cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>loc</th><th>updated_date</th></tr></thead><tbody><tr><td>1</td><td>ravi</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>1</td><td>ravi</td><td>chennai</td><td>2022-02-02</td></tr><tr><td>1</td><td>ravi</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>2</td><td>Raj</td><td>chennai</td><td>2022-02-02</td></tr><tr><td>2</td><td>Raj</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>3</td><td>Raj</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>4</td><td>Prasad</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>4</td><td>Prasad</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>5</td><td>Mahesh</td><td>chennai</td><td>2022-02-02</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ravi",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         1,
         "ravi",
         "chennai",
         "2022-02-02"
        ],
        [
         1,
         "ravi",
         "bangalore",
         "2021-01-01"
        ],
        [
         2,
         "Raj",
         "chennai",
         "2022-02-02"
        ],
        [
         2,
         "Raj",
         "bangalore",
         "2021-01-01"
        ],
        [
         3,
         "Raj",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         4,
         "Prasad",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         4,
         "Prasad",
         "bangalore",
         "2021-01-01"
        ],
        [
         5,
         "Mahesh",
         "chennai",
         "2022-02-02"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loc",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "updated_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dublicate_df = spark.read.csv(\"/scenarios/duplicates.csv\", inferSchema=True, header=True)\n",
    "dublicate_df.orderBy(col(\"id\"), col(\"updated_date\").desc()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baba3ae9-5608-4283-87d2-61c64d193754",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>loc</th><th>updated_date</th></tr></thead><tbody><tr><td>2</td><td>Raj</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>1</td><td>ravi</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>1</td><td>ravi</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>4</td><td>Prasad</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>4</td><td>Prasad</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>3</td><td>Raj</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>2</td><td>Raj</td><td>chennai</td><td>2022-02-02</td></tr><tr><td>1</td><td>ravi</td><td>chennai</td><td>2022-02-02</td></tr><tr><td>5</td><td>Mahesh</td><td>chennai</td><td>2022-02-02</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "Raj",
         "bangalore",
         "2021-01-01"
        ],
        [
         1,
         "ravi",
         "bangalore",
         "2021-01-01"
        ],
        [
         1,
         "ravi",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         4,
         "Prasad",
         "bangalore",
         "2021-01-01"
        ],
        [
         4,
         "Prasad",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         3,
         "Raj",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         2,
         "Raj",
         "chennai",
         "2022-02-02"
        ],
        [
         1,
         "ravi",
         "chennai",
         "2022-02-02"
        ],
        [
         5,
         "Mahesh",
         "chennai",
         "2022-02-02"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loc",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "updated_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>loc</th><th>updated_date</th></tr></thead><tbody><tr><td>2</td><td>Raj</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>1</td><td>ravi</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>1</td><td>ravi</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>4</td><td>Prasad</td><td>bangalore</td><td>2021-01-01</td></tr><tr><td>4</td><td>Prasad</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>3</td><td>Raj</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>2</td><td>Raj</td><td>chennai</td><td>2022-02-02</td></tr><tr><td>1</td><td>ravi</td><td>chennai</td><td>2022-02-02</td></tr><tr><td>5</td><td>Mahesh</td><td>chennai</td><td>2022-02-02</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "Raj",
         "bangalore",
         "2021-01-01"
        ],
        [
         1,
         "ravi",
         "bangalore",
         "2021-01-01"
        ],
        [
         1,
         "ravi",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         4,
         "Prasad",
         "bangalore",
         "2021-01-01"
        ],
        [
         4,
         "Prasad",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         3,
         "Raj",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         2,
         "Raj",
         "chennai",
         "2022-02-02"
        ],
        [
         1,
         "ravi",
         "chennai",
         "2022-02-02"
        ],
        [
         5,
         "Mahesh",
         "chennai",
         "2022-02-02"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loc",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "updated_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we get same output\n",
    "# that means we are not remove Duplicates\n",
    "dublicate_df.dropDuplicates().display()\n",
    "dublicate_df.distinct().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb20a48-c1e6-4fc2-ab50-259629c8c804",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>loc</th><th>updated_date</th></tr></thead><tbody><tr><td>1</td><td>ravi</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>2</td><td>Raj</td><td>chennai</td><td>2022-02-02</td></tr><tr><td>3</td><td>Raj</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>4</td><td>Prasad</td><td>Hyderabad</td><td>2022-06-10</td></tr><tr><td>5</td><td>Mahesh</td><td>chennai</td><td>2022-02-02</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ravi",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         2,
         "Raj",
         "chennai",
         "2022-02-02"
        ],
        [
         3,
         "Raj",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         4,
         "Prasad",
         "Hyderabad",
         "2022-06-10"
        ],
        [
         5,
         "Mahesh",
         "chennai",
         "2022-02-02"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loc",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "updated_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to remove duplicates we use column name\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "dublicate_df.orderBy(col(\"updated_date\").desc()).dropDuplicates([\"id\"]).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e65135-96a1-43d8-8de4-a9fe0aa5390a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# we can do this using window function (row_number())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b72edd8e-4a21-42ed-bd9d-d8a606d8bad5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "df = dublicate_df.withColumn(\"rowid\",row_number().over(Window.partitionBy(\"id\").orderBy(col(\"updated_date\").desc())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48560365-1638-402e-9a27-5091906dae5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>loc</th><th>updated_date</th><th>rowid</th></tr></thead><tbody><tr><td>1</td><td>ravi</td><td>Hyderabad</td><td>2022-06-10</td><td>1</td></tr><tr><td>2</td><td>Raj</td><td>chennai</td><td>2022-02-02</td><td>1</td></tr><tr><td>3</td><td>Raj</td><td>Hyderabad</td><td>2022-06-10</td><td>1</td></tr><tr><td>4</td><td>Prasad</td><td>Hyderabad</td><td>2022-06-10</td><td>1</td></tr><tr><td>5</td><td>Mahesh</td><td>chennai</td><td>2022-02-02</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ravi",
         "Hyderabad",
         "2022-06-10",
         1
        ],
        [
         2,
         "Raj",
         "chennai",
         "2022-02-02",
         1
        ],
        [
         3,
         "Raj",
         "Hyderabad",
         "2022-06-10",
         1
        ],
        [
         4,
         "Prasad",
         "Hyderabad",
         "2022-06-10",
         1
        ],
        [
         5,
         "Mahesh",
         "chennai",
         "2022-02-02",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loc",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "updated_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "rowid",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.filter(\"rowid=1\").display() # here we get all unique data according to condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6c1c4f3-6ba4-4856-af2e-fdeb2e66bf0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>loc</th><th>updated_date</th><th>rowid</th></tr></thead><tbody><tr><td>1</td><td>ravi</td><td>chennai</td><td>2022-02-02</td><td>2</td></tr><tr><td>1</td><td>ravi</td><td>bangalore</td><td>2021-01-01</td><td>3</td></tr><tr><td>2</td><td>Raj</td><td>bangalore</td><td>2021-01-01</td><td>2</td></tr><tr><td>4</td><td>Prasad</td><td>bangalore</td><td>2021-01-01</td><td>2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "ravi",
         "chennai",
         "2022-02-02",
         2
        ],
        [
         1,
         "ravi",
         "bangalore",
         "2021-01-01",
         3
        ],
        [
         2,
         "Raj",
         "bangalore",
         "2021-01-01",
         2
        ],
        [
         4,
         "Prasad",
         "bangalore",
         "2021-01-01",
         2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "loc",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"yyyy-M-d\"}",
         "name": "updated_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "rowid",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.filter(\"rowid>1\").display() # here we get data that contain duplicates"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1191367209944402,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "how to remove duplicate rows from dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
