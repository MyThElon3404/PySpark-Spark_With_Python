{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f077c57-2036-465d-af00-fc99d804569e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1055290162342735#setting/sparkui/1211-085921-k4yzhap9/driver-7892971823657005834\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0c97303550>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe31ceff-72a5-442d-a6d7-611f78bd7eb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-247691655964474>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mput\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/FileStore/tables/channels.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[38;5;124;43mCHANNEL_ID,CHANNEL_DESC,CHANNEL_CLASS,CHANNEL_CLASS_ID,CHANNEL_TOTAL,CHANNEL_TOTAL_ID\u001B[39;49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m3,Direct Sales,Direct,12,Channel total,1\u001B[39;49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m9,Tele Sales,Direct,12,Channel total,1\u001B[39;49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m5,Catalog,Indirect,13,Channel total,1\u001B[39;49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m4,Internet,Indirect,13,Channel total,1\u001B[39;49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m2,Partners,Others,14,Channel total,1\u001B[39;49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m12,Partners,Others,14,Channel total,1,45,ram,3434\u001B[39;49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43msample,Partners,Others,14,Channel total,1,45,ram,3434\u001B[39;49m\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;124;43m10 Partners Others 14 Channel total 1\u001B[39;49m\n",
       "\u001B[1;32m     10\u001B[0m \u001B[38;5;124;43m11 Partners Others 14 Channel total 1\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:378\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    376\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    377\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o1789.put.\n",
       ": org.apache.hadoop.fs.FileAlreadyExistsException: s3a://databricks-prod-storage-oregon/devtierprod1/1055290162342735/FileStore/tables/channels.csv already exists\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:124)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendNonIdempotent(DbfsClient.scala:82)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.createUnbuffered(DatabricksFileSystemV1.scala:127)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.create(DatabricksFileSystemV1.scala:109)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:142)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1233)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1210)\n",
       "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1091)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$3(DBUtilsCore.scala:482)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:146)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$2(DBUtilsCore.scala:479)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:141)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$1(DBUtilsCore.scala:479)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:70)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:70)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:70)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:70)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:134)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.put(DBUtilsCore.scala:478)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-247691655964474>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mput\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/FileStore/tables/channels.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[38;5;124;43mCHANNEL_ID,CHANNEL_DESC,CHANNEL_CLASS,CHANNEL_CLASS_ID,CHANNEL_TOTAL,CHANNEL_TOTAL_ID\u001B[39;49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;43m3,Direct Sales,Direct,12,Channel total,1\u001B[39;49m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;43m9,Tele Sales,Direct,12,Channel total,1\u001B[39;49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;43m5,Catalog,Indirect,13,Channel total,1\u001B[39;49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;43m4,Internet,Indirect,13,Channel total,1\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;43m2,Partners,Others,14,Channel total,1\u001B[39;49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;43m12,Partners,Others,14,Channel total,1,45,ram,3434\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;43msample,Partners,Others,14,Channel total,1,45,ram,3434\u001B[39;49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;43m10 Partners Others 14 Channel total 1\u001B[39;49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;43m11 Partners Others 14 Channel total 1\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:378\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    376\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    377\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o1789.put.\n: org.apache.hadoop.fs.FileAlreadyExistsException: s3a://databricks-prod-storage-oregon/devtierprod1/1055290162342735/FileStore/tables/channels.csv already exists\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:124)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendNonIdempotent(DbfsClient.scala:82)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.createUnbuffered(DatabricksFileSystemV1.scala:127)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.create(DatabricksFileSystemV1.scala:109)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:142)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1233)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1210)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1091)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$3(DBUtilsCore.scala:482)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:146)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$2(DBUtilsCore.scala:479)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:141)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$put$1(DBUtilsCore.scala:479)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:668)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:686)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:216)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:70)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:472)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:70)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:663)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:582)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:70)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:573)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:542)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:70)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:134)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.put(DBUtilsCore.scala:478)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.hadoop.fs.FileAlreadyExistsException: s3a://databricks-prod-storage-oregon/devtierprod1/1055290162342735/FileStore/tables/channels.csv already exists",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.put(\"/FileStore/tables/channels.csv\",\"\"\"CHANNEL_ID,CHANNEL_DESC,CHANNEL_CLASS,CHANNEL_CLASS_ID,CHANNEL_TOTAL,CHANNEL_TOTAL_ID\n",
    "3,Direct Sales,Direct,12,Channel total,1\n",
    "9,Tele Sales,Direct,12,Channel total,1\n",
    "5,Catalog,Indirect,13,Channel total,1\n",
    "4,Internet,Indirect,13,Channel total,1\n",
    "2,Partners,Others,14,Channel total,1\n",
    "12,Partners,Others,14,Channel total,1,45,ram,3434\n",
    "sample,Partners,Others,14,Channel total,1,45,ram,3434\n",
    "10 Partners Others 14 Channel total 1\n",
    "11 Partners Others 14 Channel total 1\"\"\")\n",
    "\n",
    "# PERMISSIVE (default): nulls are inserted for fields that could not be parsed correctly\n",
    "# DROPMALFORMED: drops lines that contain fields that could not be parsed\n",
    "# FAILFAST: aborts the reading if any malformed data is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "578a18d7-b097-49af-bde1-edf3b585d75f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_channel = spark.read.csv(\"/FileStore/tables/channels.csv\", header=True, inferSchema=True)\n",
    "df_channel.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5617927b-bc1c-4d47-9435-446190d26546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_channel.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90ca7b89-cd51-4f67-92b7-d118b58bb983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "sc = StructType([\n",
    "    StructField('CHANNEL_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_DESC', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_TOTAL', StringType(), True), \n",
    "    StructField('CHANNEL_TOTAL_ID', IntegerType(), True)\n",
    "])\n",
    "df_channel1 = spark.read.schema(sc).option(\"mode\", \"PERMISSIVE\").csv(\"/FileStore/tables/channels.csv\", header=True)\n",
    "df_channel1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1eb278-c51c-462e-9bef-e2b03eaab8bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "sc = StructType([\n",
    "    StructField('CHANNEL_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_DESC', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_TOTAL', StringType(), True), \n",
    "    StructField('CHANNEL_TOTAL_ID', IntegerType(), True)\n",
    "])\n",
    "df_channel1 = spark.read.schema(sc).option(\"mode\", \"FAILFAST\").csv(\"/FileStore/tables/channels.csv\", header=True)\n",
    "df_channel1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf385f91-a0c1-49ea-a64a-feaf346ee896",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "sc = StructType([\n",
    "    StructField('CHANNEL_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_DESC', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_TOTAL', StringType(), True), \n",
    "    StructField('CHANNEL_TOTAL_ID', IntegerType(), True)\n",
    "])\n",
    "df_channel1 = spark.read.schema(sc).option(\"mode\", \"DROPMALFORMED\").csv(\"/FileStore/tables/channels.csv\", header=True)\n",
    "df_channel1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd87294-6e0f-40ed-9773-c8a1f484cc00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CHANNEL_ID</th><th>CHANNEL_DESC</th><th>CHANNEL_CLASS</th><th>CHANNEL_CLASS_ID</th><th>CHANNEL_TOTAL</th><th>CHANNEL_TOTAL_ID</th></tr></thead><tbody><tr><td>3</td><td>Direct Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td></tr><tr><td>9</td><td>Tele Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td></tr><tr><td>5</td><td>Catalog</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td></tr><tr><td>4</td><td>Internet</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td></tr><tr><td>2</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Direct Sales",
         "Direct",
         12,
         "Channel total",
         1
        ],
        [
         9,
         "Tele Sales",
         "Direct",
         12,
         "Channel total",
         1
        ],
        [
         5,
         "Catalog",
         "Indirect",
         13,
         "Channel total",
         1
        ],
        [
         4,
         "Internet",
         "Indirect",
         13,
         "Channel total",
         1
        ],
        [
         2,
         "Partners",
         "Others",
         14,
         "Channel total",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CHANNEL_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_DESC",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# another way to handle bad recods is store them at particular location\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "sc = StructType([\n",
    "    StructField('CHANNEL_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_DESC', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_TOTAL', StringType(), True), \n",
    "    StructField('CHANNEL_TOTAL_ID', IntegerType(), True)\n",
    "])\n",
    "df_channel1 = spark.read.schema(sc).option(\"badRecordsPath\", \"/channels/badata/\").csv(\"/FileStore/tables/channels.csv\", header=True)\n",
    "df_channel1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9dc5a55-d788-4b20-ab9f-07d3d1bded35",
     "showTitle": true,
     "title": " let's check for bad records"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/channels/badata/20231211T092420/bad_records/part-00000-bd4d11a1-c38b-4204-a163-bfd1355d278b</td><td>part-00000-bd4d11a1-c38b-4204-a163-bfd1355d278b</td><td>1040</td><td>1702286662000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/channels/badata/20231211T092420/bad_records/part-00000-bd4d11a1-c38b-4204-a163-bfd1355d278b",
         "part-00000-bd4d11a1-c38b-4204-a163-bfd1355d278b",
         1040,
         1702286662000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "ls /channels/badata/20231211T092420/bad_records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8981eb82-cae3-40a9-a804-95ab06d497b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;12,Partners,Others,14,Channel total,1,45,ram,3434&quot;,&quot;reason&quot;:&quot;org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 12,Partners,Others,14,Channel total,1,45,ram,3434 SQLSTATE: KD000&quot;}\n",
       "{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;sample,Partners,Others,14,Channel total,1,45,ram,3434&quot;,&quot;reason&quot;:&quot;org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: sample,Partners,Others,14,Channel total,1,45,ram,3434 SQLSTATE: KD000&quot;}\n",
       "{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;10 Partners Others 14 Channel total 1&quot;,&quot;reason&quot;:&quot;org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 10 Partners Others 14 Channel total 1 SQLSTATE: KD000&quot;}\n",
       "{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;11 Partners Others 14 Channel total 1&quot;,&quot;reason&quot;:&quot;org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 11 Partners Others 14 Channel total 1 SQLSTATE: KD000&quot;}\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;12,Partners,Others,14,Channel total,1,45,ram,3434&quot;,&quot;reason&quot;:&quot;org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 12,Partners,Others,14,Channel total,1,45,ram,3434 SQLSTATE: KD000&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;sample,Partners,Others,14,Channel total,1,45,ram,3434&quot;,&quot;reason&quot;:&quot;org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: sample,Partners,Others,14,Channel total,1,45,ram,3434 SQLSTATE: KD000&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;10 Partners Others 14 Channel total 1&quot;,&quot;reason&quot;:&quot;org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 10 Partners Others 14 Channel total 1 SQLSTATE: KD000&quot;}\n{&quot;path&quot;:&quot;dbfs:/FileStore/tables/channels.csv&quot;,&quot;record&quot;:&quot;11 Partners Others 14 Channel total 1&quot;,&quot;reason&quot;:&quot;org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 11 Partners Others 14 Channel total 1 SQLSTATE: KD000&quot;}\n\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs \n",
    "head dbfs:/channels/badata/20231211T092420/bad_records/part-00000-bd4d11a1-c38b-4204-a163-bfd1355d278b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28fe017a-e4a3-4867-96c9-676ebb3f685d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>reason</th><th>record</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/channels.csv</td><td>org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 12,Partners,Others,14,Channel total,1,45,ram,3434 SQLSTATE: KD000</td><td>12,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>dbfs:/FileStore/tables/channels.csv</td><td>org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: sample,Partners,Others,14,Channel total,1,45,ram,3434 SQLSTATE: KD000</td><td>sample,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>dbfs:/FileStore/tables/channels.csv</td><td>org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 10 Partners Others 14 Channel total 1 SQLSTATE: KD000</td><td>10 Partners Others 14 Channel total 1</td></tr><tr><td>dbfs:/FileStore/tables/channels.csv</td><td>org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 11 Partners Others 14 Channel total 1 SQLSTATE: KD000</td><td>11 Partners Others 14 Channel total 1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/channels.csv",
         "org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 12,Partners,Others,14,Channel total,1,45,ram,3434 SQLSTATE: KD000",
         "12,Partners,Others,14,Channel total,1,45,ram,3434"
        ],
        [
         "dbfs:/FileStore/tables/channels.csv",
         "org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: sample,Partners,Others,14,Channel total,1,45,ram,3434 SQLSTATE: KD000",
         "sample,Partners,Others,14,Channel total,1,45,ram,3434"
        ],
        [
         "dbfs:/FileStore/tables/channels.csv",
         "org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 10 Partners Others 14 Channel total 1 SQLSTATE: KD000",
         "10 Partners Others 14 Channel total 1"
        ],
        [
         "dbfs:/FileStore/tables/channels.csv",
         "org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: 11 Partners Others 14 Channel total 1 SQLSTATE: KD000",
         "11 Partners Others 14 Channel total 1"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "reason",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "record",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we check how our bad data and it's meta data get store in json format\n",
    "df_badata = spark.read.json(\"/channels/badata/*/*/\")\n",
    "df_badata.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7eff379-02bd-4125-a454-5b6b539295de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CHANNEL_ID</th><th>CHANNEL_DESC</th><th>CHANNEL_CLASS</th><th>CHANNEL_CLASS_ID</th><th>CHANNEL_TOTAL</th><th>CHANNEL_TOTAL_ID</th><th>Bad_DATA</th></tr></thead><tbody><tr><td>3</td><td>Direct Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>9</td><td>Tele Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>5</td><td>Catalog</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>4</td><td>Internet</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>2</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>null</td></tr><tr><td>12</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>12,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>null</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>sample,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>10 Partners Others 14 Channel total 1</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>11 Partners Others 14 Channel total 1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Direct Sales",
         "Direct",
         12,
         "Channel total",
         1,
         null
        ],
        [
         9,
         "Tele Sales",
         "Direct",
         12,
         "Channel total",
         1,
         null
        ],
        [
         5,
         "Catalog",
         "Indirect",
         13,
         "Channel total",
         1,
         null
        ],
        [
         4,
         "Internet",
         "Indirect",
         13,
         "Channel total",
         1,
         null
        ],
        [
         2,
         "Partners",
         "Others",
         14,
         "Channel total",
         1,
         null
        ],
        [
         12,
         "Partners",
         "Others",
         14,
         "Channel total",
         1,
         "12,Partners,Others,14,Channel total,1,45,ram,3434"
        ],
        [
         null,
         "Partners",
         "Others",
         14,
         "Channel total",
         1,
         "sample,Partners,Others,14,Channel total,1,45,ram,3434"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         "10 Partners Others 14 Channel total 1"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         "11 Partners Others 14 Channel total 1"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CHANNEL_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_DESC",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Bad_DATA",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# another method is create a column and store badate in that column\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "sc = StructType([\n",
    "    StructField('CHANNEL_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_DESC', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS', StringType(), True), \n",
    "    StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n",
    "    StructField('CHANNEL_TOTAL', StringType(), True), \n",
    "    StructField('CHANNEL_TOTAL_ID', IntegerType(), True),\n",
    "    StructField('Bad_DATA', StringType(), True)\n",
    "])\n",
    "df_channel1 = spark.read.schema(sc).csv(\"/FileStore/tables/channels.csv\", header=True, columnNameOfCorruptRecord=\"Bad_DATA\")\n",
    "df_channel1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1131a09c-5191-4942-9e07-74f7612c6493",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Data -> \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CHANNEL_ID</th><th>CHANNEL_DESC</th><th>CHANNEL_CLASS</th><th>CHANNEL_CLASS_ID</th><th>CHANNEL_TOTAL</th><th>CHANNEL_TOTAL_ID</th></tr></thead><tbody><tr><td>3</td><td>Direct Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td></tr><tr><td>9</td><td>Tele Sales</td><td>Direct</td><td>12</td><td>Channel total</td><td>1</td></tr><tr><td>5</td><td>Catalog</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td></tr><tr><td>4</td><td>Internet</td><td>Indirect</td><td>13</td><td>Channel total</td><td>1</td></tr><tr><td>2</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Direct Sales",
         "Direct",
         12,
         "Channel total",
         1
        ],
        [
         9,
         "Tele Sales",
         "Direct",
         12,
         "Channel total",
         1
        ],
        [
         5,
         "Catalog",
         "Indirect",
         13,
         "Channel total",
         1
        ],
        [
         4,
         "Internet",
         "Indirect",
         13,
         "Channel total",
         1
        ],
        [
         2,
         "Partners",
         "Others",
         14,
         "Channel total",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CHANNEL_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_DESC",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL_ID",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# filtering good data and bad data\n",
    "good_data = df_channel1.filter(\"Bad_DATA is null\").drop(\"Bad_DATA\")\n",
    "print(\"Good Data -> \")\n",
    "good_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e436837b-0588-4781-9f06-39ea0a40a615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>CHANNEL_ID</th><th>CHANNEL_DESC</th><th>CHANNEL_CLASS</th><th>CHANNEL_CLASS_ID</th><th>CHANNEL_TOTAL</th><th>CHANNEL_TOTAL_ID</th><th>Bad_DATA</th></tr></thead><tbody><tr><td>12</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>12,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>null</td><td>Partners</td><td>Others</td><td>14</td><td>Channel total</td><td>1</td><td>sample,Partners,Others,14,Channel total,1,45,ram,3434</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>10 Partners Others 14 Channel total 1</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>11 Partners Others 14 Channel total 1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         12,
         "Partners",
         "Others",
         14,
         "Channel total",
         1,
         "12,Partners,Others,14,Channel total,1,45,ram,3434"
        ],
        [
         null,
         "Partners",
         "Others",
         14,
         "Channel total",
         1,
         "sample,Partners,Others,14,Channel total,1,45,ram,3434"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         "10 Partners Others 14 Channel total 1"
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         "11 Partners Others 14 Channel total 1"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "CHANNEL_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_DESC",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_CLASS_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "CHANNEL_TOTAL_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Bad_DATA",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_channel1.filter(\"Bad_DATA is not null\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8927279-00d8-4938-adb2-c5eca1bd565c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Data -> \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Bad_DATA</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Bad_DATA",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_data = df_channel1.filter(\"Bad_DATA is not null\").select(\"Bad_DATA\")\n",
    "print(\"Bad Data -> \")\n",
    "bad_data.display()\n",
    "\n",
    "# i don't know why output is not shown here"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 247691655964485,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "How to Handle Bad Data in pyspark dataframe",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
